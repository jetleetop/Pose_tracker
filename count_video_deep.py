import cv2
import numpy as np
import mediapipe as mp
from tensorflow.keras.models import load_model
from collections import deque

# 1. ëª¨ë¸ ë° MediaPipe ì´ˆê¸°í™”
model = load_model('exercise_phase_count_model.keras')
mp_pose = mp.solutions.pose
pose = mp_pose.Pose(min_detection_confidence=0.7, min_tracking_confidence=0.7)

# 2. ë¹„ë””ì˜¤ ì„¤ì •
cap = cv2.VideoCapture('test_video.mp4')
WIDTH = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
HEIGHT = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = cap.get(cv2.CAP_PROP_FPS)

# 3. ë¶„ì„ ë³€ìˆ˜ ì´ˆê¸°í™”
sequence = deque(maxlen=30)  # 30í”„ë ˆì„ ì‹œí€€ìŠ¤
count = 0
state_history = []
phase_names = ['idle', 'crunching', 'top']

# 4. ìƒíƒœ ë¨¸ì‹  ì„¤ì •
STATE = {
    "prev_phase": 0,
    "cool_down": 0,
    "current_count": 0
}
current_phase = 0
count_prob = 0.0
# 5. ë©”ì¸ ë£¨í”„
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # 5-1. ê´€ì ˆ ì¶”ì •
    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = pose.process(img_rgb)

    if results.pose_landmarks:
        # 5-2. í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ (RIGHT_ELBOW, LEFT_KNEE ë“± 4ê°œ ê´€ì ˆ)
        keypoints = []
        for joint in [mp_pose.PoseLandmark.RIGHT_ELBOW,
                      mp_pose.PoseLandmark.RIGHT_KNEE,
                      mp_pose.PoseLandmark.LEFT_ELBOW,
                      mp_pose.PoseLandmark.LEFT_KNEE]:
            lm = results.pose_landmarks.landmark[joint]
            keypoints.extend([lm.x, lm.y])

        sequence.append(keypoints)

        # 5-3. ì˜ˆì¸¡ ìˆ˜í–‰ (30í”„ë ˆì„ ì±„ì›Œì§ˆ ë•Œë§ˆë‹¤)
        if len(sequence) == 30:
            input_data = np.array(sequence).reshape(1, 30, 8)
            phase_pred, count_pred = model.predict(input_data, verbose=0)

            current_phase = np.argmax(phase_pred[0])
            count_prob = count_pred[0][0]

            # 5-4. ìƒíƒœ ì „ì´ì— ë”°ë¥¸ ì¹´ìš´íŠ¸ ì²˜ë¦¬
            if (STATE["cool_down"] <= 0 and
                    current_phase == 2 and  # top ìƒíƒœ
                    STATE["prev_phase"] == 1):  # crunching â†’ top ì „í™˜

                STATE["current_count"] += 1
                STATE["cool_down"] = int(fps * 0.5)  # 0.5ì´ˆ ì¿¨ë‹¤ìš´

            STATE["prev_phase"] = current_phase
            STATE["cool_down"] = max(0, STATE["cool_down"] - 1)
            state_history.append(current_phase)

    # 5-5. ì‹œê°í™”
    # ê´€ì ˆ ì—°ê²°ì„  ê·¸ë¦¬ê¸°
    if results.pose_landmarks:
        for connection in mp_pose.POSE_CONNECTIONS:
            start = connection[0]
            end = connection[1]
            if start in [mp_pose.PoseLandmark.RIGHT_ELBOW, mp_pose.PoseLandmark.LEFT_ELBOW,
                         mp_pose.PoseLandmark.RIGHT_KNEE, mp_pose.PoseLandmark.LEFT_KNEE]:
                start_point = (int(results.pose_landmarks.landmark[start].x * WIDTH),
                               int(results.pose_landmarks.landmark[start].y * HEIGHT))
                end_point = (int(results.pose_landmarks.landmark[end].x * WIDTH),
                             int(results.pose_landmarks.landmark[end].y * HEIGHT))
                cv2.line(frame, start_point, end_point, (0, 255, 0), 2)

    # ìƒíƒœ ë° ì¹´ìš´íŠ¸ í‘œì‹œ
    cv2.putText(frame, f"Phase: {phase_names[current_phase]}", (20, 50),
                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
    cv2.putText(frame, f"Count: {STATE['current_count']}", (20, 100),
                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 3)
    cv2.putText(frame, f"Confidence: {count_prob:.2f}", (20, 150),
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (200, 200, 0), 2)

    # 5-6. ì¶œë ¥
    cv2.imshow('Exercise Analysis', frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# 6. ì¢…ë£Œ ì²˜ë¦¬
cap.release()
cv2.destroyAllWindows()

# 7. ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸
print(f"\nğŸ“Š ìµœì¢… ë¶„ì„ ê²°ê³¼")
print(f"- ì´ ìš´ë™ íšŸìˆ˜: {STATE['current_count']}")
print(f"- ìƒíƒœ ë¶„í¬: idle({state_history.count(0)}), crunching({state_history.count(1)}), top({state_history.count(2)})")